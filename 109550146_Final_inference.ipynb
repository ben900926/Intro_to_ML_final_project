{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben900926/Intro_to_ML_final_project/blob/main/109550146_Final_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modules"
      ],
      "metadata": {
        "id": "IffldYJ9QLTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download input data from my drive\n",
        "!gdown \"1GDmfpyUQJSR30OQGop1OfEPEos9sndrl\" # train.csv https://drive.google.com/file/d/1GDmfpyUQJSR30OQGop1OfEPEos9sndrl/view?usp=sharing\n",
        "!gdown \"1JBxfTEXZCGWfmKFQWUlEcwlBYVnIRcUh\" # test.csv https://drive.google.com/file/d/1JBxfTEXZCGWfmKFQWUlEcwlBYVnIRcUh/view?usp=sharing\n",
        "# download the MODEL WEIGHT from my drive \n",
        "!gdown \"1-5d3PfMkJr7ln3xu5xQbZQsqlqqInJbq\" # link: https://drive.google.com/file/d/1-5d3PfMkJr7ln3xu5xQbZQsqlqqInJbq/view?usp=sharing\n",
        "# sample submission.csv\n",
        "!gdown \"1-5xIIYDvwWfR0JSWqd5LNFX7K_mWLldN\" # dont_click_me.csv https://drive.google.com/file/d/1-5xIIYDvwWfR0JSWqd5LNFX7K_mWLldN/view?usp=sharing\n",
        "\n",
        "# download encoder needed for preprocessing\n",
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "fcCkiLm5QNDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f40991-0c00-4106-fe1f-10c95d018c38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GDmfpyUQJSR30OQGop1OfEPEos9sndrl\n",
            "To: /content/train.csv\n",
            "100% 3.95M/3.95M [00:00<00:00, 136MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JBxfTEXZCGWfmKFQWUlEcwlBYVnIRcUh\n",
            "To: /content/test.csv\n",
            "100% 3.06M/3.06M [00:00<00:00, 171MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-5d3PfMkJr7ln3xu5xQbZQsqlqqInJbq\n",
            "To: /content/final_model.pkl\n",
            "100% 330/330 [00:00<00:00, 514kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-5xIIYDvwWfR0JSWqd5LNFX7K_mWLldN\n",
            "To: /content/dont_click_me.csv\n",
            "100% 532k/532k [00:00<00:00, 106MB/s]\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.5.1.post0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2022.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.5.1.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression, HuberRegressor, LogisticRegression\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# auc metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Pipeline Constructors\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import FunctionTransformer, RobustScaler\n",
        "from category_encoders import WOEEncoder\n",
        "import sklearn\n",
        "# save model\n",
        "import pickle"
      ],
      "metadata": {
        "id": "qOtQrcrVSehV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "8UuVjSA0QPKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 153\n",
        "\n",
        "########## please offer the link to other \"final_model.pkl\" IF you run my train code! ####################\n",
        "MODEL_WEIGHT_PATH = \"final_model.pkl\"\n",
        "########################################################################################################################\n",
        "\n",
        "# submission.csv file path\n",
        "SUBMISSION_PATH = \"109550146_submission.csv\"\n",
        "# sample submission"
      ],
      "metadata": {
        "id": "ZAz_mBDLQTdf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "TRlxnX0nsYXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read input data\n",
        "train_df = pd.read_csv(\"train.csv\") \n",
        "target = train_df.pop(\"failure\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# use 2 production groups for valid set\n",
        "production = test_df[\"product_code\"].unique()\n",
        "# pick 2 index out of five product codes\n",
        "cmb_groups = list(itertools.combinations(production, 2))"
      ],
      "metadata": {
        "id": "xiCT2EkNRm80"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(df_train, df_test):\n",
        "    data = pd.concat([df_train, df_test])\n",
        "    \n",
        "    # new attribute inspired by disscusion\n",
        "    data['area'] = data['attribute_2'] * data['attribute_3']\n",
        "    data['m3_null'] = data['measurement_3'].isnull().astype(np.int64)\n",
        "    data['m5_null'] = data['measurement_5'].isnull().astype(np.int64)\n",
        "    feature = [f for f in df_test.columns if f.startswith('measurement') or f =='loading']\n",
        "\n",
        "    full_fill_dict ={}\n",
        "    full_fill_dict['measurement_17'] = {\n",
        "        'A': [                'measurement_4','measurement_5','measurement_6','measurement_7','measurement_8'],\n",
        "        'B': ['measurement_3','measurement_4','measurement_5',                'measurement_7',                'measurement_9'],\n",
        "        'C': [                                'measurement_5','measurement_6','measurement_7','measurement_8','measurement_9'],\n",
        "        'D': ['measurement_3',                'measurement_5','measurement_6','measurement_7','measurement_8'],\n",
        "        'E': [                'measurement_4','measurement_5','measurement_6',                'measurement_8','measurement_9'],\n",
        "        'F': [                'measurement_4','measurement_5','measurement_6','measurement_7'],\n",
        "        'G': [                'measurement_4','measurement_5','measurement_6',                'measurement_8','measurement_9'],\n",
        "        'H': [                'measurement_4','measurement_5',                'measurement_7','measurement_8','measurement_9'],\n",
        "        'I': ['measurement_3','measurement_4',                'measurement_7','measurement_8','measurement_9']\n",
        "    }\n",
        "\n",
        "    # features without measurement\n",
        "    col = [col for col in df_test.columns if 'measurement' not in col]+ ['loading', 'm3_null', 'm5_null']\n",
        "    a = []\n",
        "    b = []\n",
        "\n",
        "    for x in range(3,17):\n",
        "      # correlation between each measurement and measurement 3~17\n",
        "      corr = np.absolute(data.drop(col, axis=1).corr()[f'measurement_{x}']).sort_values(ascending=False)\n",
        "      # sorted\n",
        "      corr = corr.sort_values(ascending=False)\n",
        "      a.append(np.round(np.sum(corr[1:4]),3))\n",
        "      b.append(f'measurement_{x}')\n",
        "\n",
        "    # making df with correlations\n",
        "    corr_df = pd.DataFrame()\n",
        "    corr_df['corr_sum'] = b\n",
        "    corr_df['selected_col'] = a\n",
        "    corr_df = corr_df.sort_values(by = 'corr_sum',ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # select top-10 measurement with highest correlation\n",
        "    for i in range(10):\n",
        "      measurement_col = 'measurement_' + corr_df.iloc[i,0][12:]\n",
        "      fill_dict = {}\n",
        "      # correlation for each production code\n",
        "      for x in data.product_code.unique() : \n",
        "          #print(measurement_col)\n",
        "          corr = np.absolute(data[data.product_code == x].drop(col, axis=1).corr()[measurement_col]).sort_values(ascending=False)\n",
        "          # {'measurement_8': ['measurement_17', 'measurement_2', 'measurement_0', 'measurement_3']}\n",
        "          measurement_col_dic = {}\n",
        "          measurement_col_dic[measurement_col] = corr[1:5].index.tolist()\n",
        "          fill_dict[x] = measurement_col_dic[measurement_col]\n",
        "      full_fill_dict[measurement_col] =fill_dict\n",
        "\n",
        "    feature = [f for f in data.columns if f.startswith('measurement') or f=='loading']\n",
        "    nullValue_cols = [col for col in df_train.columns if df_train[col].isnull().sum()!=0]\n",
        "\n",
        "    for code in data.product_code.unique():\n",
        "      # we are using high-correlated given measurements to predict missing measurement_col\n",
        "      for measurement_col in list(full_fill_dict.keys()):\n",
        "        # train model with non-null\n",
        "        tmp = data[data.product_code == code]\n",
        "        column = full_fill_dict[measurement_col][code]\n",
        "        tmp_train = tmp[column+[measurement_col]].dropna(how='any')\n",
        "        measurement_null = (tmp[column].isnull().sum(axis=1) == 0) & (tmp[measurement_col].isnull())\n",
        "        tmp_test = tmp[measurement_null]\n",
        "\n",
        "        # using HugerRegressor(linear regression that is robust to outlines) to predict missing value\n",
        "        model = HuberRegressor(epsilon=1.9, max_iter=500)\n",
        "        model.fit(tmp_train[column], tmp_train[measurement_col])\n",
        "        measure_null_only = (data.product_code==code) & (data[column].isnull().sum(axis=1)==0) & (data[measurement_col].isnull())\n",
        "        data.loc[measure_null_only, measurement_col] = model.predict(tmp_test[column])\n",
        "\n",
        "      # now using KNN imputer to impute missing values\n",
        "      model1 = KNNImputer(n_neighbors=3)\n",
        "      data.loc[data.product_code==code, feature] = model1.fit_transform(data.loc[data.product_code==code, feature])\n",
        "\n",
        "    # average of measurement\n",
        "    data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n",
        "    df_train = data.iloc[:df_train.shape[0],:]\n",
        "    df_test = data.iloc[df_train.shape[0]:,:]\n",
        "    features = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', 'measurement_1', 'measurement_2', 'area', 'm3_null', 'm5_null', 'measurement_avg']\n",
        "    \n",
        "    return df_train, df_test, features"
      ],
      "metadata": {
        "id": "CSJUZUEZscP3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocess\n",
        "x_train, x_test, features = preprocessing(train_df, test_df)\n",
        "\n",
        "# pipeline (to make preprocessing faster)\n",
        "preprocessing_pp = make_pipeline(\n",
        "    make_column_transformer(\n",
        "        (WOEEncoder(), ['attribute_0']), # turn string into binary value\n",
        "        (FunctionTransformer(np.log1p), ['loading']),\n",
        "        remainder = 'passthrough'\n",
        "    ),\n",
        "    RobustScaler()\n",
        ")\n",
        "\n",
        "# cross validation\n",
        "# use 2 production groups for valid set\n",
        "production = train_df[\"product_code\"].unique()\n",
        "# pick 2 index out of five product codes\n",
        "cmb_groups = list(itertools.combinations(production, 2))\n",
        "\n",
        "train_index = []\n",
        "valid_index = []\n",
        "\n",
        "# pick out data using these codes\n",
        "for group in cmb_groups:\n",
        "  group_zero_list = train_df.loc[train_df[\"product_code\"]==group[0], :].index\n",
        "  group_one_list = train_df.loc[train_df[\"product_code\"]==group[1], :].index\n",
        "  # combine two list\n",
        "  list0 = list(group_zero_list)\n",
        "  list1 = list(group_one_list)\n",
        "  tmp_list = list0 + list1\n",
        "\n",
        "  # total list - test set = train set\n",
        "  train_set = set(list(train_df.index)) - set(tmp_list)\n",
        "  train_index.append(list(train_set))\n",
        "  valid_index.append(tmp_list)"
      ],
      "metadata": {
        "id": "6RU6xNEQs43G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Prediction"
      ],
      "metadata": {
        "id": "zxRvQ6vTQB7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "def set_seeds(seed):\n",
        "  random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "# main train loop here\n",
        "def score(input_model):\n",
        "  set_seeds(seed=SEED)\n",
        "\n",
        "  # store some results\n",
        "  test_preds = np.zeros((x_test.shape[0],)) \n",
        "\n",
        "  for fold, train_ in enumerate(train_index):\n",
        "    # training data\n",
        "    x_train_ = x_train[features].iloc[train_, :].copy()\n",
        "    y_train = target[train_].copy()\n",
        "\n",
        "    # define model and train\n",
        "    model = make_pipeline(\n",
        "        clone(preprocessing_pp),\n",
        "        clone(input_model)\n",
        "    )\n",
        "\n",
        "    model.fit(x_train_, y_train)\n",
        "    # get predictions\n",
        "\n",
        "    test_preds += model.predict_proba(x_test)[:,1] / len(train_index)\n",
        "\n",
        "  return test_preds"
      ],
      "metadata": {
        "id": "QaKtmObeQiDu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "with open(MODEL_WEIGHT_PATH, \"rb\") as pklFile:\n",
        "  model = pickle.load(pklFile)\n",
        "\n",
        "# making submission\n",
        "submission = pd.read_csv(\"dont_click_me.csv\")\n",
        "submission['failure'] = score(\n",
        "    model\n",
        ")\n",
        "\n",
        "submission.to_csv(SUBMISSION_PATH, index=False)"
      ],
      "metadata": {
        "id": "5ZuXaDd4QA0W"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}